--- load options ---
batch_size: 2
concat: 1
crop_size: 216
d_iter: 3
dataroot: datasets/mini
dis_norm: None
dis_scale: 3
dis_spectral_norm: False
display_dir: DISPLAY
display_freq: 10
gaussian_size: 64
gpu: 0
img_save_freq: 5
input_dim: 3
isDcontent: True
lambda_cls: 1.0
lambda_cls_G: 5.0
lambda_rec: 10
lr_policy: lambda
model_save_freq: 10
nThreads: 8
n_ep: 1200
n_ep_decay: 600
name: outputs
no_display_img: False
no_flip: False
num_classes: 2
num_domains: 2
phase: train
resize_size: 256
result_dir: RESULT
resume: None
x_dim: 262144

--- load dataset ---

--- load model ---
x_dim 64 2 64
start the training at epoch 0

--- train ---
c tensor([[0., 1.],
        [1., 0.]], device='cuda:0')
i torch.Size([2, 3, 216, 216])
c tensor([[0., 1.],
        [1., 0.]], device='cuda:0')
i torch.Size([2, 3, 216, 216])
c tensor([[1., 0.],
        [1., 0.]], device='cuda:0')
i torch.Size([2, 3, 216, 216])
z torch.Size([2, 256, 54, 54])
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=64, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=64, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
inf {'mean': tensor([[-0.0438,  0.0395, -0.0364,  0.0251,  0.0179, -0.0094,  0.0269,  0.0122,
          0.0088, -0.0636, -0.0597,  0.0080, -0.0352,  0.0291,  0.0380, -0.0414,
         -0.0444,  0.0178,  0.0382,  0.0093, -0.0087,  0.0133,  0.0287,  0.0364,
         -0.0041,  0.0546,  0.0487, -0.0206,  0.0541,  0.0214,  0.0527, -0.0429,
         -0.0025,  0.0134,  0.0633, -0.0407, -0.0021, -0.0587,  0.0316,  0.0638,
         -0.0082, -0.0150, -0.0500,  0.0307, -0.0278, -0.0704,  0.0334, -0.0028,
         -0.0083,  0.0304,  0.0079, -0.0115, -0.0301,  0.0012, -0.0045,  0.0095,
         -0.0368,  0.0532,  0.0511,  0.0362,  0.0223,  0.0553,  0.0326,  0.0101],
        [-0.0334,  0.0386, -0.0390,  0.0297,  0.0163, -0.0237,  0.0143,  0.0162,
          0.0132, -0.0544, -0.0618,  0.0058, -0.0210,  0.0305,  0.0308, -0.0339,
         -0.0465,  0.0269,  0.0568,  0.0161, -0.0088,  0.0174,  0.0343,  0.0328,
         -0.0090,  0.0352,  0.0414, -0.0274,  0.0584,  0.0210,  0.0544, -0.0363,
          0.0020,  0.0095,  0.0545, -0.0418, -0.0022, -0.0568,  0.0275,  0.0411,
          0.0041, -0.0229, -0.0525,  0.0331, -0.0182, -0.0695,  0.0355, -0.0059,
         -0.0083,  0.0245,  0.0184, -0.0186, -0.0269, -0.0101, -0.0060,  0.0109,
         -0.0358,  0.0362,  0.0405,  0.0449,  0.0217,  0.0645,  0.0175,  0.0152]],
       device='cuda:0', grad_fn=<AddmmBackward0>), 'var': tensor([[0.7076, 0.6903, 0.6904, 0.6643, 0.6848, 0.6767, 0.6848, 0.6872, 0.7019,
         0.6954, 0.7175, 0.6773, 0.6813, 0.6985, 0.7022, 0.7065, 0.6822, 0.6642,
         0.6973, 0.6978, 0.6870, 0.7133, 0.6707, 0.6946, 0.7054, 0.7007, 0.6986,
         0.6915, 0.6774, 0.6805, 0.6871, 0.7139, 0.6794, 0.6789, 0.7304, 0.7081,
         0.7045, 0.6808, 0.6761, 0.6764, 0.6721, 0.6948, 0.6980, 0.7069, 0.7175,
         0.7113, 0.6715, 0.6940, 0.7339, 0.6793, 0.6699, 0.7026, 0.6749, 0.6587,
         0.7000, 0.7050, 0.6850, 0.6895, 0.6841, 0.7031, 0.7163, 0.6889, 0.6938,
         0.7003],
        [0.7062, 0.6881, 0.6987, 0.6698, 0.6838, 0.6805, 0.6853, 0.6914, 0.7009,
         0.6993, 0.7170, 0.6720, 0.6857, 0.6915, 0.6983, 0.7148, 0.6891, 0.6743,
         0.6988, 0.6930, 0.6889, 0.7047, 0.6751, 0.7000, 0.7057, 0.6985, 0.6964,
         0.6920, 0.6759, 0.6820, 0.6814, 0.7101, 0.6860, 0.6853, 0.7317, 0.7044,
         0.7071, 0.6820, 0.6719, 0.6829, 0.6691, 0.6945, 0.6937, 0.7038, 0.7170,
         0.7132, 0.6777, 0.6893, 0.7259, 0.6807, 0.6682, 0.7079, 0.6775, 0.6540,
         0.7009, 0.7017, 0.6810, 0.7036, 0.6845, 0.7063, 0.7072, 0.6882, 0.7006,
         0.6937]], device='cuda:0', grad_fn=<SoftplusBackward0>), 'gaussian': tensor([[-0.6411,  0.0715, -0.2101,  1.1489, -0.2655, -0.3966,  0.7200,  0.9262,
          1.1075,  0.1824,  0.2699,  0.3565,  0.4703, -0.4429, -0.2281,  1.1802,
          0.0977,  0.2276, -1.1060, -0.8137, -0.1846,  0.2937, -0.3174,  0.6227,
          0.6473,  0.0600,  0.9636,  0.7558,  0.5486,  0.1834,  0.2088, -0.7474,
         -0.4251, -0.2440,  0.2818,  0.9377, -0.7284,  0.0093,  0.3077, -0.8501,
         -0.7429,  0.0275,  0.4215, -0.9408,  2.1529,  0.1094, -0.1734,  0.3450,
         -0.5397,  0.2450, -0.5471, -0.6900,  0.2142, -0.2406,  1.0419, -0.6636,
          0.4527, -0.0064,  0.5769, -0.2554,  0.2654, -0.5721,  1.6769,  0.5369],
        [-0.6127, -0.3696, -0.2843, -0.6609, -0.0873,  0.9034, -0.8020, -2.1277,
         -0.3329, -0.6175, -0.2268, -0.4812,  0.5745,  0.0543,  0.1644,  1.4909,
          0.2822, -0.5100,  1.4344,  0.8286, -0.9581, -0.1343,  1.2486, -0.5661,
          0.7098,  0.3689, -1.0837, -0.2800,  0.2900, -0.2826, -0.7619,  0.6425,
         -0.8688, -0.0340, -0.7280,  0.4583, -0.5905, -0.3729, -0.0085, -0.0576,
          0.6258,  0.4515, -1.2407, -0.2355,  0.6071, -0.2939, -0.2621,  0.6438,
         -0.3793, -0.0694, -0.0976, -0.9810, -0.0498,  0.2807, -0.7975,  0.1132,
          1.6935, -0.4926, -0.4089,  0.2143, -0.8203, -0.3980, -0.7953, -0.5089]],
       device='cuda:0', grad_fn=<AddBackward0>), 'logits': tensor([[0.0403, 0.0324],
        [0.0403, 0.0324]], device='cuda:0', grad_fn=<ViewBackward0>), 'prob_cat': tensor([[0.5020, 0.4980],
        [0.5020, 0.4980]], device='cuda:0', grad_fn=<SoftmaxBackward0>), 'categorical': tensor([[0.0623, 0.9377],
        [0.6004, 0.3996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)}
log torch.Size([2, 64])
z_attra torch.Size([1, 64])
z_random torch.Size([1, 64])
content torch.Size([3, 256, 54, 54])
attr torch.Size([3, 64])
c torch.Size([3, 2])
size xcz torch.Size([3, 322, 54, 54])
size xcz torch.Size([3, 322, 54, 54])
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=64, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=64, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
size xcz torch.Size([1, 322, 54, 54])
size xcz torch.Size([1, 322, 54, 54])
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=64, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
Entra in forward infNet
Entra in qyx
entra in else
layer: Linear(in_features=64, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in else
layer: Linear(in_features=512, out_features=512, bias=True)
entra in else
layer: ReLU()
entra in if
Esce da qyx
Esce da forward infNet
/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825: UserWarning: Error detected in NativeLayerNormBackward0. Traceback of forward call that caused the error:
  File "/home/davide/Greta/DRIT/MDM/MDMM/train.py", line 91, in <module>
    main()
  File "/home/davide/Greta/DRIT/MDM/MDMM/train.py", line 62, in main
    model.update_D(images, c_org)
  File "/home/davide/Greta/DRIT/MDM/MDMM/model.py", line 223, in update_D
    self.forward()
  File "/home/davide/Greta/DRIT/MDM/MDMM/model.py", line 190, in forward
    self.fake_B_recon = self.gen.forward(self.z_content_recon_b, self.z_attr_recon_b, c_org_B)
  File "/home/davide/Greta/DRIT/MDM/MDMM/networks.py", line 171, in forward
    out3 = self.dec3(x_and_z3)
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/davide/Greta/DRIT/MDM/MDMM/networks.py", line 489, in forward
    return self.model(x)
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/davide/Greta/DRIT/MDM/MDMM/networks.py", line 351, in forward
    return F.layer_norm(x, normalized_shape, self.weight.expand(normalized_shape), self.bias.expand(normalized_shape))
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/nn/functional.py", line 2900, in layer_norm
    return torch.layer_norm(
 (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:110.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/home/davide/Greta/DRIT/MDM/MDMM/train.py", line 91, in <module>
    main()
  File "/home/davide/Greta/DRIT/MDM/MDMM/train.py", line 63, in main
    model.update_EG()
  File "/home/davide/Greta/DRIT/MDM/MDMM/model.py", line 259, in update_EG
    self.backward_EG()
  File "/home/davide/Greta/DRIT/MDM/MDMM/model.py", line 309, in backward_EG
    loss_G.backward(retain_graph=True)
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/davide/Greta/DRIT/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 18.19 MiB is free. Including non-PyTorch memory, this process has 3.78 GiB memory in use. Of the allocated memory 3.57 GiB is allocated by PyTorch, and 136.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)